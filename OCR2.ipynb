{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f760031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd=r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35ec0f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "import re,string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4722170d",
   "metadata": {},
   "source": [
    "### Extracting text for one meme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44eeadf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A42sky42 said\\n\\nMy English teacher says we shouldn't\\nrefer to authors by their first names\\nbecause they aren't our friends. Will you\\nconfirm our friendship and let me call\\nyou Neil on my American Gods book\\nreport?\\n\\n(2) neil-gaiman\\n\\nAbsolutely.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(r\"memes/2.png\")\n",
    "\n",
    "text=pytesseract.image_to_string(image)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d69e00",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3d5e46",
   "metadata": {},
   "source": [
    "The first function, fetch_memes(), takes a directory path as an input and reads in all the images in that directory using the cv2 library. It then extracts text from each image using the pytesseract library and appends the extracted text to a list. Finally, it returns a Pandas DataFrame containing the list of extracted texts.\n",
    "\n",
    "The second function, standardization(), takes the DataFrame returned by fetch_memes and performs several standardization steps on the text. First, it converts all text to lowercase using the lower() method. Next, it removes all numbers using a regular expression (re.sub(r'\\d+', '', x)). It then replaces all newline and tab characters with a space. It also removes any occurrences of .com from the start or end of the text string. Finally, it removes all punctuation using the translate() method and returns the standardized DataFrame.\n",
    "\n",
    "The third function, label_memes(), takes the standardized DataFrame returned by standardization and adds a new column called labels. For each meme in the DataFrame, it uses a pre-trained sentiment analysis model (jayanta/ProsusAI-finbert-sentiment-finetuned-memes) to classify the meme as either positive or negative. It then appends the label to a list and adds that list as a new column to the DataFrame. The function returns the labeled DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b81a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_memes(directory_in_str):\n",
    "    directory = os.fsencode(directory_in_str)\n",
    "    list_of_texts=[]\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        img=cv2.imread(os.path.join(os.fsdecode(directory),filename))\n",
    "        text=pytesseract.image_to_string(img)\n",
    "        list_of_texts.append(text)\n",
    "\n",
    "    data = pd.DataFrame(list_of_texts)\n",
    "    \n",
    "    return data\n",
    "\n",
    "        \n",
    "def standardization(data):\n",
    "    data = data[0]\n",
    "    data = data.apply(lambda x: x.lower()) #converting to lowercase\n",
    "    data = data.apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "    data = data.apply(lambda x: re.sub(r'\\n', ' ', x))\n",
    "    data = data.apply(lambda x: re.sub(r'\\t', ' ', x))\n",
    "    data = data.apply(lambda x: re.sub(r'.com', '', x, flags=re.MULTILINE)) #removing .coms from the start or end of the text string. \n",
    "    # flags=re.MULTILINE is a flag that allows the ^ and $ characters to match the start and end of each line\n",
    "    data = data.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation))) #removes all punctuations\n",
    "    data = pd.DataFrame(data)\n",
    "    return data\n",
    "\n",
    "def label_memes(data):\n",
    "    labels=[]\n",
    "    classifier = pipeline('sentiment-analysis',model=\"jayanta/ProsusAI-finbert-sentiment-finetuned-memes\")\n",
    "    for meme in data[0]:\n",
    "        l=classifier(meme)[0]['label']\n",
    "        if l=='0':\n",
    "            labels.append('Positive')\n",
    "        else:\n",
    "            labels.append('Negative')\n",
    "    \n",
    "    data['labels']=labels\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41cf77f",
   "metadata": {},
   "source": [
    "### HuggingFace functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8439292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard(text):\n",
    "    x = text.lower()\n",
    "    x = re.sub(r'\\d+', '', x)\n",
    "    x = re.sub(r'\\n', ' ', x)\n",
    "    x = re.sub(r'\\t', ' ', x)\n",
    "    x = re.sub(r'.com', '', x, flags=re.MULTILINE)\n",
    "    x = x.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    return x\n",
    "    \n",
    "def predict(text):\n",
    "    \n",
    "    output=dict()\n",
    "    classifier = pipeline('sentiment-analysis',model=\"jayanta/ProsusAI-finbert-sentiment-finetuned-memes\")\n",
    "    l=classifier(text)[0]['label']\n",
    "    p=classifier(text)[0]['score']\n",
    "    \n",
    "    if l=='0':\n",
    "        output['Positive']=p\n",
    "    else:\n",
    "        output['Negative']=p\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "765f89b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OCR(directory_in_str):\n",
    "    data = fetch_memes(directory_in_str)\n",
    "    df = standardization(data)\n",
    "    labels = label_memes(df)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2901bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Result=OCR(r\"memes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5723715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '0', 'score': 0.9999703168869019}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis',model=\"jayanta/ProsusAI-finbert-sentiment-finetuned-memes\")\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a54bb9",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17f2edc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ratatouille  cook me dinner you fucking useles...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asky said  my english teacher says we shouldnt...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>here is a new challenge for all you bored teen...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>took my  year old to a classmate’s birthday pa...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my grandpa who knows nothing about video games...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>go to the same gas station everyday after wo...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>myatose fathers  feeding tube before ramadan</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>javell tm jvizzle  d how is it “get over slave...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>enough about my racist past</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>found my clone selling nuts in  istanbul  we c...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>news  world  europe  coronavirus austria bans ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>end of the worldes in   the dyslexic prophet w...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>what unrealistic things in movies bug the hell...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ayod  wxuous       top proof that we are nota ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>your mom gets anew boyfriend  he’s a gamer  he...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>teacher hypes up class party all year  the par...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>me  hours ahead in asia knowing the election r...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>go ahead rs  c  a   they cannot unshit your gr...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>d my adopted dad explaining why  me waiting fo...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>the sun  prettycooltim   ‘finger es cine  clos...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>we dio iy reddit  rblackpeopletwitter  rwhitep...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>the nazis surrendered may th   a  ‘ig chuck no...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hours of child labor  in the world this year ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ratatouille  cook me dinner you fucking useles...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>uu ln joke about  ei  at will be funy</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>friends of journalism journ  d © do you agree ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>so you could be watching it  in  years’ time e...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0    labels\n",
       "0   ratatouille  cook me dinner you fucking useles...  Negative\n",
       "1   asky said  my english teacher says we shouldnt...  Positive\n",
       "2   here is a new challenge for all you bored teen...  Positive\n",
       "3   took my  year old to a classmate’s birthday pa...  Negative\n",
       "4   my grandpa who knows nothing about video games...  Negative\n",
       "5     go to the same gas station everyday after wo...  Positive\n",
       "6       myatose fathers  feeding tube before ramadan   Positive\n",
       "7   javell tm jvizzle  d how is it “get over slave...  Negative\n",
       "8                       enough about my racist past    Negative\n",
       "9   found my clone selling nuts in  istanbul  we c...  Negative\n",
       "10  news  world  europe  coronavirus austria bans ...  Negative\n",
       "11  end of the worldes in   the dyslexic prophet w...  Positive\n",
       "12  what unrealistic things in movies bug the hell...  Negative\n",
       "13  ayod  wxuous       top proof that we are nota ...  Negative\n",
       "14  your mom gets anew boyfriend  he’s a gamer  he...  Positive\n",
       "15  teacher hypes up class party all year  the par...  Positive\n",
       "16  me  hours ahead in asia knowing the election r...  Negative\n",
       "17  go ahead rs  c  a   they cannot unshit your gr...  Negative\n",
       "18  d my adopted dad explaining why  me waiting fo...  Negative\n",
       "19  the sun  prettycooltim   ‘finger es cine  clos...  Positive\n",
       "20  we dio iy reddit  rblackpeopletwitter  rwhitep...  Positive\n",
       "21  the nazis surrendered may th   a  ‘ig chuck no...  Negative\n",
       "22   hours of child labor  in the world this year ...  Negative\n",
       "23  ratatouille  cook me dinner you fucking useles...  Negative\n",
       "24             uu ln joke about  ei  at will be funy   Positive\n",
       "25  friends of journalism journ  d © do you agree ...  Negative\n",
       "26  so you could be watching it  in  years’ time e...  Negative"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c82f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5edcf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodings=tokenizer(Xp,truncation=True,padding=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
